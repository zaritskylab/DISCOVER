{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/IShengFang/SpectralNormalizationKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG and CLF only \n",
    "# 350\n",
    "# cov loss\n",
    "# fid2 loss ( CLF flatten(real) , CLF flatten(real) )\n",
    "# z clf score\n",
    "# hard VGG images\n",
    "# disc1D BN and dropout\n",
    "# remove MMD/fid \n",
    "# GaussianNoise only resblock down\n",
    "# zmean->0 zvar->1 as loss \n",
    "# zscore bce\n",
    "#unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 20:30:36.632649: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-23 20:30:37.700857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Lambda, Input, GaussianNoise,concatenate, Dense, Dropout, Conv2D, Add, UpSampling2D, Dot, Conv2DTranspose, Activation, Reshape, InputSpec, LeakyReLU, Flatten, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K  \n",
    "# from keras.utils.generic_utils import Progbar\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "# from keras.engine import *\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "# from keras.utils.generic_utils import func_dump\n",
    "# from keras.utils.generic_utils import func_load\n",
    "# from keras.utils.generic_utils import deserialize_keras_object\n",
    "# from keras.utils.generic_utils import has_arg\n",
    "# from keras.utils import conv_utils\n",
    "from keras.layers import Dense, Conv1D, Conv2D, Conv3D, Conv2DTranspose, Embedding\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from keras.models import load_model\n",
    "# import tensorflow_probability as tfp\n",
    "from keras.applications.vgg19 import VGG19\n",
    "# from tensorflow_addons.layers import SpectralNormalization\n",
    "from keras.activations import swish\n",
    "from keras.initializers import glorot_uniform, glorot_normal, GlorotUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_path = '/sise/assafzar-group/assafzar/oded_R/DISCOVER/ALTZHEIMER_MRI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperperemeter\n",
    "BATCHSIZE=64\n",
    "LEARNING_RATE = 0.0002 # initial 0.0002\n",
    "TRAINING_RATIO = 1\n",
    "BETA_1 = 0.0\n",
    "BETA_2 = 0.9\n",
    "EPOCHS = 500\n",
    "BN_MIMENTUM = 0.1\n",
    "BN_EPSILON  = 0.00002 \n",
    "latent_dim = 350\n",
    "img_size = 64\n",
    "channels = 1\n",
    "img_shape = (img_size, img_size, channels)\n",
    "nmax = 200000\n",
    "ker_init = GlorotUniform(seed=None)\n",
    "noise_var = 1\n",
    "\n",
    "load_model_true = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (6101, 64, 64, 1)\n",
      "X_test (299, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "margin=20\n",
    "def load_images(directory):\n",
    "    imagePaths = list(paths.list_images(directory))\n",
    "    data = []\n",
    "    images_name = []\n",
    "    i = 1\n",
    "    for imagePath in imagePaths:\n",
    "        images_name.append(imagePath.split(\"/\")[-1])\n",
    "        image = cv2.imread(imagePath) # load the image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) # swap color channels\n",
    "        image = image[margin:image.shape[0]-margin , margin:image.shape[1]-margin] \n",
    "        image = cv2.resize(image, (img_size,img_size)) \n",
    "        data.append(image)\n",
    "        i = i+1\n",
    "        if i>nmax:\n",
    "            break\n",
    "    data_arr = np.array(data)\n",
    "    req_images = np.array(data)\n",
    "    req_images =  np.array(req_images).astype(\"float32\")\n",
    "    req_images = np.expand_dims(req_images, axis=3)\n",
    "    return req_images , images_name\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "# train\n",
    "images1, names1 = load_images(domain_path + 'data/train/NonDemented') # ('/home/odedrot/images/ccf_images_64pix_train')\n",
    "images0, names0 = load_images(domain_path + 'data/train/MildDemented') # ('/home/odedrot/images/Unet_images_64pix_train')  \n",
    "images2, names2 = load_images(domain_path + 'data/train/VeryMildDemented') # ('/home/odedrot/images/ccf_images_64pix_train')\n",
    "\n",
    "X_train = np.concatenate((images0, images1, images2))\n",
    "\n",
    "################################################################################################\n",
    "# test\n",
    "images1, names1 = load_images(domain_path + 'data/test/NonDemented') # ('/home/odedrot/images/Unet_images_64pix_clf_bc_test')\n",
    "images0, names0 = load_images(domain_path + 'data/test/MildDemented') # ('/home/odedrot/images/Unet_images_64pix_clf_aa_test')\n",
    "images2, names2 = load_images(domain_path + 'data/test/VeryMildDemented') # ('/home/odedrot/images/Unet_images_64pix_clf_aa_test')\n",
    "\n",
    "X_test = np.concatenate((images0, images1, images2)) \n",
    "names_test = np.concatenate((names0, names1, names2))\n",
    "\n",
    "print('X_train' , X_train.shape)\n",
    "print('X_test' , X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brightness_aug(img, b_val):\n",
    "    img = img + b_val\n",
    "    img = np.clip(img, 0, 1) \n",
    "    return img\n",
    "def rotate_aug(img, angle):\n",
    "    h, w = img_size, img_size\n",
    "    center = (w / 2, h / 2)\n",
    "    scale = 1\n",
    "    M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    rotated = cv2.warpAffine(img, M, (w, h))\n",
    "    return rotated\n",
    "def flip_ver(img):\n",
    "    flipVertical = cv2.flip(img, 0)\n",
    "    return flipVertical\n",
    "def flip_hor(img):\n",
    "    flipHorizontal = cv2.flip(img, 1)\n",
    "    return flipHorizontal\n",
    "def noise_aug(img):\n",
    "    original_img = np.copy(img)\n",
    "    noise_mean = 0\n",
    "    noise_var = 0.001\n",
    "    noise_img = np.random.normal(noise_mean,noise_var, size=(img.shape[0], img.shape[0]))\n",
    "    img = img + noise_img\n",
    "    img = np.clip(img, 0, 1) \n",
    "    img[original_img<0.01] = 0 \n",
    "    return img\n",
    "\n",
    "def augment_img(img):\n",
    "    aug_img = np.copy(img)\n",
    "    # rotate\n",
    "    rand_val = np.random.uniform(0,1)\n",
    "    if rand_val > 0.5:\n",
    "        angle = np.round(tf.random.shuffle(np.linspace(90, 270, num=3))[0].numpy(),3) # np.linspace(-15, 15, num=31\n",
    "        aug_img = rotate_aug(aug_img, angle)\n",
    "    # brightness\n",
    "    rand_val = np.random.uniform(0,1)\n",
    "    if rand_val > 0.5:\n",
    "        b_val = np.round(tf.random.shuffle(np.linspace(-0.2, 0.2, num=100))[0].numpy(),3)\n",
    "        aug_img = brightness_aug(aug_img, b_val)\n",
    "    # flip hor\n",
    "    rand_val = np.random.uniform(0,1)\n",
    "    if rand_val > 0.5:\n",
    "        aug_img = flip_hor(aug_img)\n",
    "    # flip ver\n",
    "    rand_val = np.random.uniform(0,1)\n",
    "    if rand_val > 0.5:\n",
    "        aug_img = flip_ver(aug_img)\n",
    "    # noise\n",
    "    rand_val = np.random.uniform(0,1)\n",
    "    if rand_val > 0.5:\n",
    "        aug_img = noise_aug(aug_img)\n",
    "    return aug_img\n",
    "\n",
    "def augment_imgs(imgs):\n",
    "    aug_imgs = []\n",
    "    for i in range(len(imgs)):\n",
    "        aug_imgs.append(augment_img(imgs[i][:,:,0]))\n",
    "    aug_imgs = np.array(aug_imgs)\n",
    "    aug_imgs = np.expand_dims(aug_imgs, axis=3)\n",
    "    return aug_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(imgs):\n",
    "    r, c = 2, 10\n",
    "    fig, axs = plt.subplots(r, c, figsize=(20,4))\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseSN(Dense):\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                                 initializer=initializers.RandomNormal(0, 1),\n",
    "                                 name='sn',\n",
    "                                 trainable=False)\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                 W_bar = K.reshape(W_bar, W_shape)  \n",
    "        output = K.dot(inputs, W_bar)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class ConvSN2D(Conv2D):\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                         initializer=initializers.RandomNormal(0, 1),\n",
    "                         name='sn',\n",
    "                         trainable=False)\n",
    "        \n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "    def call(self, inputs, training=None):\n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            #Accroding the paper, we only need to do power iteration one time.\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        #Spectral Normalization\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                W_bar = K.reshape(W_bar, W_shape)\n",
    "                \n",
    "        outputs = K.conv2d(\n",
    "                inputs,\n",
    "                W_bar,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs    \n",
    "\n",
    "    \n",
    "class ConvSN2DTranspose(Conv2DTranspose):\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) != 4:\n",
    "            raise ValueError('Inputs should have rank ' +\n",
    "                             str(4) +\n",
    "                             '; Received input shape:', str(input_shape))\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (self.filters, input_dim)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                         initializer=initializers.RandomNormal(0, 1),\n",
    "                         name='sn',\n",
    "                         trainable=False)\n",
    "        \n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n",
    "        self.built = True  \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_shape = K.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        if self.data_format == 'channels_first':\n",
    "            h_axis, w_axis = 2, 3\n",
    "        else:\n",
    "            h_axis, w_axis = 1, 2\n",
    "\n",
    "        height, width = input_shape[h_axis], input_shape[w_axis]\n",
    "        kernel_h, kernel_w = self.kernel_size\n",
    "        stride_h, stride_w = self.strides\n",
    "        if self.output_padding is None:\n",
    "            out_pad_h = out_pad_w = None\n",
    "        else:\n",
    "            out_pad_h, out_pad_w = self.output_padding\n",
    "\n",
    "        # Infer the dynamic output shape:\n",
    "        out_height = conv_utils.deconv_length(height,\n",
    "                                              stride_h, kernel_h,\n",
    "                                              self.padding,\n",
    "                                              out_pad_h)\n",
    "        out_width = conv_utils.deconv_length(width,\n",
    "                                             stride_w, kernel_w,\n",
    "                                             self.padding,\n",
    "                                             out_pad_w)\n",
    "        if self.data_format == 'channels_first':\n",
    "            output_shape = (batch_size, self.filters, out_height, out_width)\n",
    "        else:\n",
    "            output_shape = (batch_size, out_height, out_width, self.filters)\n",
    "            \n",
    "        #Spectral Normalization    \n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            #Accroding the paper, we only need to do power iteration one time.\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                W_bar = K.reshape(W_bar, W_shape)\n",
    "        self.kernel = W_bar\n",
    "        \n",
    "        outputs = K.conv2d_transpose(\n",
    "            inputs,\n",
    "            self.kernel,\n",
    "            output_shape,\n",
    "            self.strides,\n",
    "            padding=self.padding,\n",
    "            data_format=self.data_format)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussianNoise_std = 0.03\n",
    "\n",
    "def res_block_down(layer_input, filters):\n",
    "    d = BatchNormalization()(layer_input)\n",
    "    d = Activation('relu')(d)\n",
    "    d = Conv2D(filters, kernel_size=3, strides=2, padding='same')(d)\n",
    "    d = GaussianNoise(stddev=GaussianNoise_std)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Activation('relu')(d)\n",
    "    d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "    d = GaussianNoise(stddev=GaussianNoise_std)(d)\n",
    "    d_res = Conv2D(filters, kernel_size=1, strides=2, padding='same')(layer_input)\n",
    "    d = Add()([d, d_res])\n",
    "    return d\n",
    "\n",
    "def res_block_up(layer_input, filters):\n",
    "    d = BatchNormalization()(layer_input)\n",
    "    d = Activation('relu')(d)\n",
    "    d = UpSampling2D(size=(2, 2), interpolation=\"nearest\")(d)\n",
    "    d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "    d = GaussianNoise(stddev=GaussianNoise_std)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Activation('relu')(d)\n",
    "    d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "    d = GaussianNoise(stddev=GaussianNoise_std)(d)\n",
    "    d_res = UpSampling2D(size=(2, 2), interpolation=\"nearest\")(layer_input)\n",
    "    d_res = Conv2D(filters, kernel_size=1, strides=1, padding='same')(d_res)\n",
    "    d = Add()([d, d_res])\n",
    "    return d\n",
    "\n",
    "def BuildEncoder():\n",
    "    enc_input = Input(shape=img_shape, name='encoder_input') \n",
    "    X = Conv2D(64, kernel_size=3, padding='same', activation='relu')(enc_input)\n",
    "    X = res_block_down(X, 128)\n",
    "    X = res_block_down(X, 256)\n",
    "    X = res_block_down(X, 512)\n",
    "    X = res_block_down(X, 512)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = GaussianNoise(stddev=GaussianNoise_std)(X)\n",
    "    X = swish(X) #############\n",
    "        \n",
    "    z = Dense(latent_dim)(X)\n",
    "    z = GaussianNoise(stddev=GaussianNoise_std)(z)\n",
    "    z = BatchNormalization()(z) # (None, 350)\n",
    "    \n",
    "    intermediate = tf.keras.layers.Reshape((350,1), input_shape=(350,))(z) # (None, 350, 1)\n",
    "    first_half = tf.keras.layers.Cropping1D(cropping=(0,350-14))(intermediate) # (None, 14, 1)\n",
    "    z_split = tf.keras.layers.Reshape((14,), input_shape=(14,1))(first_half) # (None, 14)\n",
    "    \n",
    "    # z_split = Lambda( lambda x: tf.split(x,num_or_size_splits=latent_dim//14,axis=1))(z) # list of vectors of size (None, 14) each\n",
    "\n",
    "    clfscore = Dense(1, activation='sigmoid')(z_split) # z_split[0]\n",
    "    \n",
    "    encoder = Model(enc_input, [z, clfscore], name='encoder') # \n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def BuildGenerator():\n",
    "    dec_input = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "    last_conv_shape = (4, 4, 512)\n",
    "    X = Dense(last_conv_shape[0] * last_conv_shape[1] * last_conv_shape[2])(dec_input)\n",
    "    X = GaussianNoise(stddev=GaussianNoise_std)(X)\n",
    "    X = Reshape((last_conv_shape[0], last_conv_shape[1], last_conv_shape[2]))(X)\n",
    "    \n",
    "    X = res_block_up(X, 512)\n",
    "    X = res_block_up(X, 512)\n",
    "    X = res_block_up(X, 256)\n",
    "    X = res_block_up(X, 128)\n",
    "    \n",
    "    X = Conv2D(1, (3, 3), strides = (1,1), padding = 'same', kernel_initializer = ker_init)(X)\n",
    "    X = Activation('sigmoid')(X)\n",
    "    decoder = Model(dec_input, X, name='decoder')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tf.keras.Input(shape=(500,)) # (None, 500)\n",
    "# intermediate = tf.keras.layers.Dense(500,activation=tf.nn.relu)(inputs) # (None, 500)\n",
    "# intermediate = tf.keras.layers.Reshape((500,1), input_shape=(500,))(intermediate) # (None, 500, 1)\n",
    "# first_half = tf.keras.layers.Cropping1D(cropping=(0,200))(intermediate) # (None, 300, 1)\n",
    "# first_half = tf.keras.layers.Reshape((300,), input_shape=(300,1))(first_half) # (None, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildDiscriminator1D():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, input_dim=latent_dim))\n",
    "    model.add(BatchNormalization(scale=False))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add((Dense(2048)))\n",
    "    model.add(BatchNormalization(scale=False))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add((Dense(2048)))\n",
    "    model.add(BatchNormalization(scale=False))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add((Dense(2048)))\n",
    "    model.add(BatchNormalization(scale=False))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add((Dense(2048)))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add((Dense(2048)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    encoded_repr = Input(shape=(latent_dim, ))\n",
    "    validity = model(encoded_repr)\n",
    "    return Model(encoded_repr, validity, name='discriminator')\n",
    "\n",
    "def build_recognizer():\n",
    "    recognizer_input = Input(shape=img_shape, name='recognizer_input') # batchx128x128x1    \n",
    "    x = Conv2D(32, 5, strides=2, padding=\"same\", name=\"rec_conv1_preact\")(recognizer_input)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = Conv2D(64, 5, strides=2, padding=\"same\", name=\"rec_conv2_preact\")(x) \n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = Conv2D(128, 5, strides=2, padding=\"same\", name=\"rec_conv3_preact\")(x)  \n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = Conv2D(256, 5, strides=2, padding=\"same\", name=\"rec_conv4_preact\")(x)  \n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    recognizer_output = Dense(latent_dim, activation='softmax')(x)\n",
    "    recognizer_value = Dense(1)(x)\n",
    "    recognizer = Model(recognizer_input, recognizer_output, name=\"recognizer\")\n",
    "    return recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildUNet(): # https://github.com/zhixuhao/unet/blob/master/model.py\n",
    "    inputs = Input(img_shape) \n",
    "    conv1i = Conv2D(64, 3, activation = 'relu', strides=1, padding = 'same', kernel_initializer = 'he_normal')(inputs) # 64\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', strides=2, padding = 'same', kernel_initializer = 'he_normal')(conv1i) # 32\n",
    "    conv2i = Conv2D(128, 3, activation = 'relu', strides=1, padding = 'same', kernel_initializer = 'he_normal')(conv1) # 32\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', strides=2, padding = 'same', kernel_initializer = 'he_normal')(conv1) # 16\n",
    "    conv3i = Conv2D(256, 3, activation = 'relu', strides=1, padding = 'same', kernel_initializer = 'he_normal')(conv2) # 16\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', strides=2, padding = 'same', kernel_initializer = 'he_normal')(conv2) # 8\n",
    "    conv4i = Conv2D(512, 3, activation = 'relu', strides=1, padding = 'same', kernel_initializer = 'he_normal')(conv3) # 8\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', strides=2, padding = 'same', kernel_initializer = 'he_normal')(conv4i) # 4\n",
    "    # middle\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4) # 4\n",
    "    # up\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv5)) # 8\n",
    "    merge6 = concatenate([conv4i,up6], axis = 3) \n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3i,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2i,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1i,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "    model = Model(inputs, conv10, name='unet')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barlow_losses(z_a, z_b):\n",
    "    z_a_norm = (z_a - tf.reduce_mean(z_a, axis=0)) #/ tf.math.reduce_std(z_a, axis=0)\n",
    "    z_b_norm = (z_b - tf.reduce_mean(z_b, axis=0)) #/ tf.math.reduce_std(z_b, axis=0)\n",
    "    cov_a_b = (tf.transpose(z_a_norm) @ z_b_norm) / z_a.shape[0] ############ \n",
    "    on_diag = tf.linalg.diag_part(cov_a_b)\n",
    "    off_diag_mat = tf.linalg.set_diag(cov_a_b, tf.zeros(cov_a_b.shape[-1])  )\n",
    "    abs_off_diag_mean = tf.reduce_sum(tf.abs(off_diag_mat)) / (cov_a_b.shape[0]**2 - cov_a_b.shape[0])\n",
    "    off_diag_10_mat = off_diag_mat[0:10, 0:10]\n",
    "    abs_off_diag_10_mean = tf.reduce_sum(tf.abs(abs_off_diag_mean)) / (off_diag_mat.shape[0]**2 - off_diag_mat.shape[0])\n",
    "    # on_diag_loss = tf.reduce_sum( tf.pow(on_diag - 1, 2) ) # replace with hinge\n",
    "    \n",
    "    on_diag_loss = tf.reduce_mean( tf.nn.relu(1 - tf.sqrt(on_diag)) )\n",
    "    \n",
    "    off_diag_loss = abs_off_diag_mean\n",
    "    a_b_loss = tf.reduce_mean(tf.abs(z_a_norm - z_b_norm)**2)\n",
    "    return cov_a_b, on_diag_loss, off_diag_loss, a_b_loss, tf.reduce_mean(tf.abs(on_diag)), abs_off_diag_mean, abs_off_diag_10_mean\n",
    "\n",
    "def get_off_diag_values(x):\n",
    "    x_flat = tf.reshape(x,[-1])[:-1]\n",
    "    x_reshape = tf.reshape(x_flat,[x.shape[0]-1, x.shape[0]+1])[:, 1:]\n",
    "    off_diag_values = tf.reshape(x_reshape,[-1])\n",
    "    return off_diag_values\n",
    "def cov_loss_terms(z_batch):\n",
    "    z_batch = z_batch - tf.reduce_mean(z_batch, axis=0)\n",
    "    z_std = tf.sqrt(tf.math.reduce_variance(z_batch, axis=0) + 0.0001)\n",
    "    z_std_loss = tf.reduce_mean( tf.nn.relu(1 - z_std) )\n",
    "    cov = (tf.transpose(z_batch) @ z_batch) / (z_batch.shape[0] ) # cov = 1/N*(z.T*z)  tfp.stats.covariance(z_batch)\n",
    "    diag_cov = tf.linalg.diag_part(cov) # vector of vars (z_std**2)  positives\n",
    "    diag_cov_mean = tf.reduce_mean(tf.abs(diag_cov))    \n",
    "    off_diag_loss    = tf.reduce_mean(tf.abs( get_off_diag_values(cov) ))\n",
    "    off_diag_mean_10 = tf.reduce_mean(tf.abs(get_off_diag_values(cov[0:10,0:10])))\n",
    "    return cov, z_std_loss, diag_cov_mean , off_diag_loss, off_diag_mean_10 \n",
    "def calc_cov_loss(y_true, ypred):\n",
    "    cov, z_std_loss, diag_cov_mean , off_diag_loss, off_diag_mean_10 = cov_loss_terms(y_true)\n",
    "    cov_loss = 0.5*diag_cov_mean + 0.5*z_std_loss\n",
    "    return cov_loss\n",
    "\n",
    "def model_perc_loss_wrapper(model):\n",
    "    def model_perc_loss(y_true, y_pred):\n",
    "        # weights = [1,1,1]\n",
    "        img_outputs = model( y_true ) \n",
    "        recon_img_outputs = model( y_pred ) \n",
    "        layers_losses = []\n",
    "        for i in range(len(recon_img_outputs)):\n",
    "            mae_losses = K.abs(img_outputs[i] - recon_img_outputs[i])\n",
    "            layers_losses.append(K.mean(mae_losses))\n",
    "        avg_loss = K.mean(tf.convert_to_tensor(layers_losses))\n",
    "        return avg_loss\n",
    "    return model_perc_loss\n",
    "\n",
    "def z_mean_var_loss(y_true, y_pred):\n",
    "    loss = K.abs(K.var(y_pred)-1) + K.abs(K.mean(y_pred))\n",
    "    return K.mean(loss)\n",
    "\n",
    "def perturbate_embedding(z_image_batch, batch_size):\n",
    "    GT_onehot_vecs = np.zeros((batch_size, latent_dim)) # vectors of zeros which will get 1 in a random dim\n",
    "    pert_onehot_vecs = np.copy(GT_onehot_vecs) # vectors of zeros which will get a value in a random dim , it is added to z in the graph\n",
    "    for i in range(batch_size):\n",
    "        pert_dim = np.random.randint(latent_dim, size=1)[0] # choose a dim\n",
    "        values = np.linspace(-1.5, 1.5, num=16)\n",
    "        pert_value = tf.random.shuffle(values)[0] # choose a value from values\n",
    "        pert_onehot_vecs[i][pert_dim] = pert_value # change z to selected value\n",
    "        GT_onehot_vecs[i][pert_dim] = 1 # put value 1 in one hot\n",
    "    pert_onehot_vecs = np.array(pert_onehot_vecs)\n",
    "    GT_onehot_vecs = np.array(GT_onehot_vecs)\n",
    "    return pert_onehot_vecs, GT_onehot_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 20:30:58.017142: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/odedrot/.conda/envs/DISCOVERtmp/lib/python3.9/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/odedrot/.conda/envs/DISCOVERtmp/lib/python3.9/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "encoder = BuildEncoder()\n",
    "generator = BuildGenerator()\n",
    "recognizer = build_recognizer()\n",
    "unet = BuildUNet()\n",
    "discriminator1D = BuildDiscriminator1D()\n",
    "\n",
    "# ae_saved_epoch = 'vgg_seed_3_epoch_4_auc_0.79_gmax_0.73_tpr_0.79_tnr_0.68_acc_0.72_f1_0.72'\n",
    "\n",
    "# ae_model_path = '/home/odedrot/saved_models_VAEGAN/AAE_opt6_retrain2/' + ae_saved_epoch + '.h5'\n",
    "# ae_model = load_model(ae_model_path , compile=False, custom_objects={'ConvSN2D': ConvSN2D , 'DenseSN': DenseSN})\n",
    "# encoder = ae_model.get_layer('encoder')\n",
    "# generator = ae_model.get_layer('decoder')\n",
    "# unet = ae_model.get_layer('unet')\n",
    "# recognizer = ae_model.get_layer('recognizer')\n",
    "# discriminator1D = ae_model.get_layer('discriminator')\n",
    "\n",
    "import keras\n",
    "@keras.saving.register_keras_serializable(package=\"my_package\", name=\"gray2rgb\")\n",
    "def gray2rgb(tensor):\n",
    "    tensor = tf.image.grayscale_to_rgb(tensor)\n",
    "    return tensor\n",
    "\n",
    "img = Input(shape=img_shape)\n",
    "z_img, z_img_score = encoder(img) \n",
    "recon_img = generator(z_img)\n",
    "recon_img_rescaled =  Lambda(gray2rgb)(recon_img)\n",
    "                    \n",
    "superimg = unet(recon_img)\n",
    "superimg_rescaled = Lambda(gray2rgb)(superimg)\n",
    " \n",
    "\n",
    "# CNN_saved_name = 'vgg_seed_3_epoch_33_auc_0.94_gmax_0.85_tpr_0.86_tnr_0.84.h5'\n",
    "CNN_saved_name = 'vgg_seed_3_epoch_64_auc_0.75_gmax_0.73_tpr_0.69_tnr_0.78_acc_0.73_f1_0.72.keras' # new\n",
    "\n",
    "CNN_clf_model_path = domain_path + 'saved_CLF_org/' + CNN_saved_name\n",
    "CNN_clf_model = load_model(CNN_clf_model_path , compile=True, custom_objects={'Sigmoid':tf.keras.models.Model})\n",
    "CNN_clf_model.trainable = False # we dont train the CNN\n",
    "recon_img_clfscore = CNN_clf_model( recon_img_rescaled ) \n",
    "\n",
    "CNN_clf_flatten = Model(inputs=CNN_clf_model.input, outputs = CNN_clf_model.get_layer('flatten').output)\n",
    "CNN_clf_flatten.trainable = False # we dont train the CNN\n",
    "\n",
    "clf_outputs = [\n",
    "               # CNN_clf_model.get_layer('block1_conv1').output,\n",
    "               # CNN_clf_model.get_layer('block1_conv2').output,\n",
    "               # CNN_clf_model.get_layer('block2_conv1').output,\n",
    "               # CNN_clf_model.get_layer('block2_conv2').output,\n",
    "               CNN_clf_model.get_layer('block3_conv1').output,\n",
    "               CNN_clf_model.get_layer('block3_conv2').output,\n",
    "               CNN_clf_model.get_layer('block3_conv3').output,\n",
    "               CNN_clf_model.get_layer('block4_conv1').output,\n",
    "               CNN_clf_model.get_layer('block4_conv2').output,\n",
    "               CNN_clf_model.get_layer('block4_conv3').output,\n",
    "               CNN_clf_model.get_layer('block5_conv1').output,\n",
    "               CNN_clf_model.get_layer('block5_conv2').output,\n",
    "               CNN_clf_model.get_layer('block5_conv3').output,\n",
    "               CNN_clf_model.get_layer('flatten').output,\n",
    "               CNN_clf_model.get_layer('dense').output,]\n",
    "                  \n",
    "CNN_clf_Model_outputs = Model(inputs=CNN_clf_model.input, outputs = clf_outputs)\n",
    "CNN_clf_Model_outputs.trainable = False # we dont train the CNN\n",
    "\n",
    "VGG_model = VGG19(weights=\"imagenet\", include_top=False, input_shape=(img_size, img_size,3)) # ResNet50 , VGG16, VGG19\n",
    "VGG_outputs = [\n",
    "               # VGG_model.get_layer('block1_conv1').output,   \n",
    "               # VGG_model.get_layer('block1_conv2').output,\n",
    "               # VGG_model.get_layer('block2_conv1').output,\n",
    "               # VGG_model.get_layer('block2_conv2').output,   \n",
    "               VGG_model.get_layer('block3_conv1').output,   \n",
    "               VGG_model.get_layer('block3_conv2').output,\n",
    "               VGG_model.get_layer('block3_conv3').output,\n",
    "               VGG_model.get_layer('block4_conv1').output,\n",
    "               VGG_model.get_layer('block4_conv2').output,\n",
    "               VGG_model.get_layer('block4_conv3').output,\n",
    "               VGG_model.get_layer('block4_conv4').output,\n",
    "               VGG_model.get_layer('block5_conv1').output,\n",
    "               VGG_model.get_layer('block5_conv2').output,\n",
    "               VGG_model.get_layer('block5_conv3').output,\n",
    "               VGG_model.get_layer('block5_conv4').output,]\n",
    "VGG_Model_outputs = Model(inputs=VGG_model.input, outputs = VGG_outputs)\n",
    "VGG_Model_outputs.trainable = False # we dont train the CNN\n",
    "\n",
    "# recognizer\n",
    "recognizer.compile(loss='categorical_crossentropy', optimizer=Adam(LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2), metrics=['accuracy'])\n",
    "pert_vec = Input(shape=(latent_dim,))\n",
    "z_img_p = Add()([z_img, pert_vec])\n",
    "generator.trainable = False\n",
    "pert_img = generator(z_img_p)\n",
    "generator.trainable = True\n",
    "onehot_dim_pred = recognizer(pert_img - recon_img)\n",
    "\n",
    "# gen images for fid2\n",
    "# noise = Input(shape=(latent_dim,))\n",
    "# gen_img = generator(noise)\n",
    "\n",
    "# disc1D \n",
    "\n",
    "discriminator1D.compile(loss='bce', optimizer=Adam(LEARNING_RATE/10, beta_1=BETA_1, beta_2=BETA_2))\n",
    "discriminator1D.trainable = False \n",
    "disc1D_output = discriminator1D(z_img)\n",
    "\n",
    "AE = Model([img, pert_vec], [recon_img_rescaled,recon_img_rescaled, superimg_rescaled, superimg_rescaled, z_img, z_img, z_img_score, disc1D_output, onehot_dim_pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#               VGG, clf, unetVGG, unetCLF,      calc_cov_loss, z_mean_var_loss,   'mse',  'bce',   'categorical_crossentropy'\n",
    "loss_weights = [2,    2,    4,        4,             0.0000001,     0.00000001,      10,     1,       1]\n",
    "AE.compile(loss=[model_perc_loss_wrapper(VGG_Model_outputs), model_perc_loss_wrapper(CNN_clf_Model_outputs), model_perc_loss_wrapper(VGG_Model_outputs), model_perc_loss_wrapper(CNN_clf_Model_outputs), calc_cov_loss, z_mean_var_loss, 'bce', 'bce', 'categorical_crossentropy'], optimizer=Adam(LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2), loss_weights=loss_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 175ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 175ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 189ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 187ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 200\n",
    "test_images = X_test[np.random.randint(0, X_test.shape[0], test_batch_size)] / 255 \n",
    "test_images3D = test_images.repeat(3, -1)\n",
    "test_images_scores = CNN_clf_model( test_images3D )\n",
    "test_noise = np.random.normal(0,noise_var, size=(test_batch_size, latent_dim))\n",
    "test_real_y = 0.9*np.ones((test_batch_size, 1))\n",
    "test_fake_y = 0.1*np.ones((test_batch_size, 1))\n",
    "\n",
    "real_y = 0.9*np.ones((BATCHSIZE, 1))\n",
    "fake_y = 0.1*np.ones((BATCHSIZE, 1))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"epoch\", epoch)\n",
    "    iteration = 0\n",
    "    for n_batch in range(len(X_train) // BATCHSIZE):\n",
    "        idx = np.random.randint(0, X_train.shape[0], 2*BATCHSIZE)\n",
    "        image_batch = np.copy(X_train[idx]) \n",
    "        image_batch = image_batch / 255 \n",
    "        \n",
    "        ## choose high perc loss images\n",
    "        recon_image_batch = generator.predict(encoder.predict(image_batch)[0], verbose = None) \n",
    "        samples_perc_loss = []\n",
    "        for i in range(2*BATCHSIZE):\n",
    "            samples_perc_loss.append( model_perc_loss_wrapper(VGG_Model_outputs)(image_batch[i:i+1].repeat(3, -1), recon_image_batch[i:i+1].repeat(3, -1)) )\n",
    "        samples_perc_loss = np.array(samples_perc_loss)\n",
    "        sorted_ind = np.argsort(samples_perc_loss)[::-1]\n",
    "        # sorted_samples_perc_loss = samples_perc_loss[sorted_ind]\n",
    "        # sorted_samples_perc_loss = sorted_samples_perc_loss[~np.isnan(sorted_samples_perc_loss)] # remove nan\n",
    "        sorted_images = image_batch[sorted_ind]\n",
    "        image_batch = sorted_images[0:BATCHSIZE]\n",
    "        ##\n",
    "        \n",
    "        image_batch3D = image_batch.repeat(3, -1) \n",
    "        imgs_clfscore = CNN_clf_model( image_batch3D ) \n",
    "        z_image_batch = encoder.predict(image_batch, verbose = None)[0]\n",
    "        discriminator1D.trainable = False\n",
    "        \n",
    "        # recognizer       \n",
    "        pert_onehot_vecs, GT_onehot_vecs = perturbate_embedding(z_image_batch, BATCHSIZE)\n",
    "        \n",
    "        AE_loss = AE.train_on_batch([image_batch, pert_onehot_vecs], [image_batch3D, image_batch3D, image_batch3D, image_batch3D, z_image_batch,z_image_batch, imgs_clfscore, real_y, GT_onehot_vecs])\n",
    "    \n",
    "        # discriminator1D\n",
    "        # discriminator1D\n",
    "        for i in range(3):  # fake (z)\n",
    "            discriminator1D.trainable = True \n",
    "            idx = np.random.randint(0, X_train.shape[0], BATCHSIZE)\n",
    "            imgs = np.copy(X_train[idx]) \n",
    "            imgs = imgs / 255 \n",
    "            z_images = encoder.predict(imgs, verbose = None)[0]\n",
    "            d_loss = discriminator1D.train_on_batch(z_images, fake_y[0:BATCHSIZE] )\n",
    "        for i in range(3): # real (n)\n",
    "            discriminator1D.trainable = True \n",
    "            noise_vecs = np.random.normal(0,noise_var, size=(BATCHSIZE, latent_dim))\n",
    "            d_loss = discriminator1D.train_on_batch(noise_vecs, real_y[0:BATCHSIZE] )\n",
    "            \n",
    "        # if iteration % 1 == 0: \n",
    "        #     discriminator1D.trainable = True \n",
    "        #     idx = np.random.randint(0, X_train.shape[0], BATCHSIZE)\n",
    "        #     imgs = np.copy(X_train[idx]) \n",
    "        #     imgs = imgs / 255 \n",
    "        #     z_images = encoder.predict(imgs)[0]\n",
    "        #     noise_vecs = np.random.normal(0,noise_var, size=(BATCHSIZE, latent_dim))\n",
    "        #     d_loss = discriminator1D.train_on_batch(np.concatenate((z_images, noise_vecs)), np.concatenate((fake_y[0:BATCHSIZE], real_y[0:BATCHSIZE])) )\n",
    "###################################################################################################  \n",
    "        # test\n",
    "        iteration = iteration + 1\n",
    "        if epoch % 10 == 0 and iteration % 90 == 0: \n",
    "            print('iteration ' , iteration)\n",
    "            test_z_images, test_z_images_score  = encoder.predict(test_images, verbose = None)\n",
    "            test_recon_images = generator.predict(test_z_images)\n",
    "            test_z_recon_images, test_z_recon_images_score = encoder.predict(test_recon_images, verbose = None)\n",
    "            test_generated_images = generator.predict(test_noise, verbose = None)\n",
    "            test_z_generated_images = encoder.predict(test_generated_images, verbose = None)[0]\n",
    "            \n",
    "            test_super_images = unet.predict(test_recon_images, verbose = None)\n",
    "            \n",
    "            test_d1D_loss_real = discriminator1D.test_on_batch(test_noise, test_real_y)\n",
    "            test_d1D_loss_fake = discriminator1D.test_on_batch(test_z_images, test_fake_y)\n",
    "            \n",
    "            ##\n",
    "            test_Perc_loss = np.round( model_perc_loss_wrapper(VGG_Model_outputs)(test_images.repeat(3, -1), test_recon_images.repeat(3, -1)).numpy()  , 3)  \n",
    "            test_CLF_loss = np.round( model_perc_loss_wrapper(CNN_clf_Model_outputs)(test_images.repeat(3, -1), test_recon_images.repeat(3, -1)).numpy()  , 3)  \n",
    "            test_Super_Perc_loss = np.round( model_perc_loss_wrapper(VGG_Model_outputs)(test_images.repeat(3, -1), test_super_images.repeat(3, -1)).numpy()  , 3)  \n",
    "            test_Super_CLF_loss = np.round( model_perc_loss_wrapper(CNN_clf_Model_outputs)(test_images.repeat(3, -1), test_super_images.repeat(3, -1)).numpy()  , 3)   \n",
    "            \n",
    "            ## CLF scores\n",
    "            test_recon_images_scores = CNN_clf_model(test_recon_images.repeat(3, -1))\n",
    "            test_super_images_scores = CNN_clf_model(test_super_images.repeat(3, -1))\n",
    "\n",
    "            \n",
    "            ## disentanglment\n",
    "            test_pert_onehot_vecs, test_GT_onehot_vecs = perturbate_embedding(test_z_images, test_batch_size)\n",
    "            test_recon_pert_imgs = generator.predict(test_z_images + test_pert_onehot_vecs, verbose = None)\n",
    "            test_diff_recon_imgs = test_recon_pert_imgs - test_recon_images\n",
    "            \n",
    "            test_AE_loss = AE.test_on_batch([test_images, test_pert_onehot_vecs], [test_images3D, test_images3D, test_images3D, test_images3D, test_z_images,test_z_images, test_images_scores, test_real_y, test_GT_onehot_vecs])\n",
    "            test_recognizer_output = recognizer.test_on_batch(test_diff_recon_imgs, test_GT_onehot_vecs)\n",
    "            test_recognizer_loss = test_recognizer_output[0].item()\n",
    "            test_recognizer_acc = test_recognizer_output[1].item()\n",
    "            \n",
    "            zscore_loss = np.round(np.mean(np.abs(test_z_images_score[:,0] - test_images_scores[:,0].numpy())**1),4)\n",
    "            test_cov_var , test_cov_mean = cov_loss_terms(test_z_images)[2].numpy() , cov_loss_terms(test_z_images)[3].numpy()\n",
    "            test_cov_loss = 1 - test_cov_var + test_cov_mean\n",
    "           \n",
    "            print('Disc1D_real' , test_d1D_loss_real)\n",
    "            print('Disc1D_fake' , test_d1D_loss_fake)\n",
    "            print('Perc_loss' , test_Perc_loss)\n",
    "            print('CLF_loss' , test_CLF_loss)\n",
    "            print('super Perc_loss' , test_Super_Perc_loss)\n",
    "            print('super CLF_loss' , test_Super_CLF_loss)\n",
    "            print('cov [1,0]' , test_cov_var , test_cov_mean)\n",
    "            print( '[z_var, z_mean]' , np.var(test_z_images) , np.mean(test_z_images) )\n",
    "            print('zscore_loss' , zscore_loss)\n",
    "            print( 'recognizer_loss',  np.round(test_recognizer_loss,5),  np.round(test_recognizer_acc,5) )\n",
    "            print('')\n",
    "            # print('z_mmd_metric_2', z_mmd_loss2(test_z_images.astype('float32'), test_noise.astype('float32')).numpy())\n",
    "            # print('MMD gen metric' , dist_sim_wrapper(CNN_clf_flatten)(test_generated_images, test_images).numpy() )\n",
    "            # print('MMD recon metric' , dist_sim_wrapper(CNN_clf_flatten)(test_recon_images, test_images).numpy() )\n",
    "            print('mae metric' , np.mean(np.abs(test_recon_images - test_images)) )\n",
    "            print('score mae metric' , np.mean(np.abs(test_images_scores - test_recon_images_scores )) )\n",
    "            print('mae(z,zrecon) metric' , np.mean(np.abs(test_z_recon_images - test_z_images)) )\n",
    "            print('mae(noise,z_gen) metric' , np.mean(np.abs(test_z_generated_images - test_noise)) )\n",
    "         \n",
    "            print('')\n",
    "            print('real')\n",
    "            sample_images(test_images)\n",
    "            print('')\n",
    "            print('recon')\n",
    "            sample_images(test_recon_images)\n",
    "            print('')\n",
    "            # print('recon')\n",
    "            # sample_images(test_super_images)\n",
    "            # print('')\n",
    "            print('gen')\n",
    "            sample_images(test_generated_images)\n",
    "            # show gen imgs with disc1D scores close to 0 (more close to z)\n",
    "            # noise_vecs_disc1D_pred = discriminator1D.predict(test_noise)[:,0]\n",
    "            # sorted_ind = np.argsort(noise_vecs_disc1D_pred) # [::-1] \n",
    "            # sorted_noise_vecs_disc1D_pred = noise_vecs_disc1D_pred[sorted_ind]\n",
    "            # sorted_test_noise = test_noise[sorted_ind] \n",
    "            # sorted_test_gen_images = generator.predict(sorted_test_noise)\n",
    "            # sample_images(sorted_test_gen_images)\n",
    "            # print('')\n",
    "            # print('gen clf-->1')\n",
    "            # show gen imgs with clf scores close to 1 \n",
    "            # test_gen_imgs_clfscore = CNN_clf_model.predict(  test_generated_images.repeat(3, -1) )[:,0]\n",
    "            # sorted_ind = np.argsort(test_gen_imgs_clfscore)[::-1] \n",
    "            # sample_images(test_generated_images[sorted_ind])\n",
    "            # # generated scores dsitribution\n",
    "            # _ = plt.hist(test_gen_imgs_clfscore.flatten(), 100)\n",
    "            # plt.title('gen imgs clf scores')\n",
    "            # plt.show()\n",
    "            \n",
    "            df = pd.concat([pd.DataFrame(test_z_images) , pd.DataFrame(test_images_scores)] , axis=1).reset_index()\n",
    "            df = df.drop( ['index'] , axis=1)\n",
    "            cor = df.corr()\n",
    "            plt.figure(figsize=(30,10))\n",
    "            plt.bar(np.array(df.columns[:-1]), cor.iloc[-1].values[:-1] , width=1) # normalized\n",
    "            plt.xlabel(\"latent dimension\", fontsize=50)\n",
    "            plt.ylabel(\"correlation \", fontsize=50)\n",
    "            plt.xticks(fontsize=30)\n",
    "            plt.yticks(fontsize=30)\n",
    "            plt.show()\n",
    "            print(np.abs(cor.iloc[-1].values[15:-1]).max())\n",
    "            print('')\n",
    "            z_imgs_ = test_z_images - np.mean(test_z_images, axis=0)\n",
    "            z_std = np.sqrt(np.var(z_imgs_, axis=0))\n",
    "            cov = (z_imgs_.T @ z_imgs_) / (z_imgs_.shape[0] )\n",
    "            plt.imshow(np.abs(cov[0:30, 0:30]))\n",
    "            plt.show()\n",
    "                        \n",
    "            ## save\n",
    "            encoder.save(         domain_path + 'saved_DISCOVER_MODEL/encoder_epoch_'  + str(epoch+1) + '_perc_' + str(test_Perc_loss) + '_CLF_' + str(test_CLF_loss) + '_zcr_' + str(zscore_loss) + '_disent_acc_' + str(np.round(test_recognizer_loss,3)) + '_disent_loss_' + str(np.round(test_recognizer_acc,5)) + '_cov_loss_' + str(np.round(test_cov_loss,5))   + '.keras', include_optimizer=False)\n",
    "            generator.save(       domain_path + 'saved_DISCOVER_MODEL/generator_epoch_'  + str(epoch+1) + '_perc_' + str(test_Perc_loss) + '_CLF_' + str(test_CLF_loss) + '_zcr_' + str(zscore_loss) + '_disent_acc_' + str(np.round(test_recognizer_loss,3)) + '_disent_loss_' + str(np.round(test_recognizer_acc,5)) + '_cov_loss_' + str(np.round(test_cov_loss,5))   + '.keras', include_optimizer=False)\n",
    "            discriminator1D.save( domain_path + 'saved_DISCOVER_MODEL/discriminator1D_epoch_'  + str(epoch+1) + '_perc_' + str(test_Perc_loss) + '_CLF_' + str(test_CLF_loss) + '_zcr_' + str(zscore_loss) + '_disent_acc_' + str(np.round(test_recognizer_loss,3)) + '_disent_loss_' + str(np.round(test_recognizer_acc,5)) + '_cov_loss_' + str(np.round(test_cov_loss,5))   + '.keras', include_optimizer=False)\n",
    "            recognizer.save(      domain_path + 'saved_DISCOVER_MODEL/recognizer_epoch_'  + str(epoch+1) + '_perc_' + str(test_Perc_loss) + '_CLF_' + str(test_CLF_loss) + '_zcr_' + str(zscore_loss) + '_disent_acc_' + str(np.round(test_recognizer_loss,3)) + '_disent_loss_' + str(np.round(test_recognizer_acc,5)) + '_cov_loss_' + str(np.round(test_cov_loss,5))   + '.keras', include_optimizer=False)\n",
    "            unet.save(            domain_path + 'saved_DISCOVER_MODEL/unet_epoch_'  + str(epoch+1) + '_perc_' + str(test_Perc_loss) + '_CLF_' + str(test_CLF_loss) + '_zcr_' + str(zscore_loss) + '_disent_acc_' + str(np.round(test_recognizer_loss,3)) + '_disent_loss_' + str(np.round(test_recognizer_acc,5)) + '_cov_loss_' + str(np.round(test_cov_loss,5))   + '.keras', include_optimizer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) // BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6101, 64, 64, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DISCOVERtmp",
   "language": "python",
   "name": "discovertmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
