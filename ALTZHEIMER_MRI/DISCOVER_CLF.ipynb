{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from keras.layers import  Input, Dense, Flatten, Dropout, Conv2D, Activation, Add, BatchNormalization, AveragePooling2D, Concatenate\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.preprocessing import image\n",
    "# from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras import activations\n",
    "from keras.losses import BinaryCrossentropy\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "from keras.models import Model, load_model\n",
    "from keras.losses import binary_crossentropy, mean_squared_error\n",
    "from keras.initializers import glorot_uniform\n",
    "import cv2\n",
    "from imutils import paths\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder , StandardScaler , PolynomialFeatures , MinMaxScaler\n",
    "# import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "\n",
    "# import tensorflow_addons as tfa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_path = '/sise/assafzar-group/assafzar/oded_R/DISCOVER/ALTZHEIMER_MRI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size, num_channels = 64,1\n",
    "equalize_classes = 0\n",
    "aug_data_flag = 0\n",
    "clf_model = 'vgg'    # inception, resnet, vgg, densenet, encoder, simple_cnn,  ccf_model\n",
    "retrain_model = 1 # 0-imagenet weights , 1- initialize from imagenet, None for random weights not \"imagenet \n",
    "seed_val = 3 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs_withLabels_3D(imgs, labels, n_examples, fig_size):\n",
    "    n = int(np.sqrt(n_examples))\n",
    "    k=0\n",
    "    fig, ax = plt.subplots(n, n, figsize=(fig_size,fig_size))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax[i, j] .imshow(imgs[k].reshape(img_size, img_size, 3), cmap='gray')\n",
    "            ax[i, j].axis('off')\n",
    "            ax[i, j].set_title(labels[k])\n",
    "            k = k + 1\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_imgs_withLabelsandpred_3D(imgs, labels, n_examples, fig_size):\n",
    "    n = int(np.sqrt(n_examples))\n",
    "    k=0\n",
    "    fig, ax = plt.subplots(n, n, figsize=(fig_size,fig_size))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax[i, j] .imshow(imgs[k].reshape(img_size, img_size, 3), cmap='gray')\n",
    "            ax[i, j].axis('off')\n",
    "            ax[i, j].set_title(labels[k])\n",
    "            k = k + 1\n",
    "    fig.tight_layout()\n",
    "    plt.show()    \n",
    "\n",
    "def plot_imgs_withLabels_1D(imgs, labels, n_examples, fig_size):\n",
    "    n = int(np.sqrt(n_examples))\n",
    "    k=0\n",
    "    fig, ax = plt.subplots(n, n, figsize=(fig_size,fig_size))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax[i, j] .imshow(imgs[k].reshape(img_size, img_size, 1), cmap='gray')\n",
    "            ax[i, j].axis('off')\n",
    "            ax[i, j].set_title(labels[k], fontsize=30)\n",
    "            k = k + 1\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# plot_imgs_withLabels(images, images_id, 144, 15)\n",
    "\n",
    "def sample_images(imgs, r,c):\n",
    "    fig, axs = plt.subplots(r, c, figsize=(20,2*r))\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()\n",
    "##############################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin=20\n",
    "def load_images(directory):\n",
    "    imagePaths = list(paths.list_images(directory))\n",
    "    data = []\n",
    "    images_name = []\n",
    "    labels = []\n",
    "    for imagePath in imagePaths:\n",
    "        # name\n",
    "        img_name = imagePath.split(\"/\")[-1]\n",
    "        images_name.append(img_name)\n",
    "        # labels\n",
    "        # if img_name[img_name.find('cat') + 4].lower() == 'a' and img_name[img_name.find('TE') + 3].lower() == 'a':\n",
    "        #     labels.append(1)\n",
    "        # else:\n",
    "        #     labels.append(0)\n",
    "        # image\n",
    "        image = cv2.imread(imagePath) # load the image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) # swap color channels \n",
    "        image = image[margin:image.shape[0]-margin , margin:image.shape[1]-margin] \n",
    "        image = cv2.resize(image, (img_size,img_size) , interpolation = cv2.INTER_AREA)\n",
    "        data.append(image)\n",
    "        \n",
    "    data_arr = np.array(data)\n",
    "    # labels_arr = np.array(labels)\n",
    "    print(data_arr.shape, np.min(data_arr), np.max(data_arr), data_arr.dtype)\n",
    "    return data_arr, images_name # , labels_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('healthy train')\n",
    "images_1, names_1                = load_images(domain_path + 'data/train/NonDemented') \n",
    "print('sick train')\n",
    "images_0, names_0                = load_images(domain_path + 'data/train/VeryMildDemented') \n",
    "print('healthy test')\n",
    "images_1_test, names_1_test = load_images(domain_path + 'data/test/NonDemented') \n",
    "print('sick test')\n",
    "images_0_test, names_0_test  = load_images(domain_path + 'data/test/VeryMildDemented') \n",
    "\n",
    "print('Done loading images')\n",
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train \n",
    "train_images_0 = images_0 \n",
    "train_images_1 = images_1\n",
    "\n",
    "### upsample class 0\n",
    "# n_upsample_0 = 500\n",
    "# rand_ind=np.random.randint(0,len(train_images_0),n_upsample_0)\n",
    "# train_images_0 =  np.concatenate(( train_images_0, train_images_0[rand_ind]))\n",
    "### upsample class 1\n",
    "# n_upsample_1 = 400\n",
    "# rand_ind=np.random.randint(0,len(train_images_1),n_upsample_1)\n",
    "# train_images_1 =  np.concatenate(( train_images_1, train_images_1[rand_ind]))\n",
    "\n",
    "\n",
    "# if equalize_classes == 1:  \n",
    "#     def euql_cls(imgs_maj, imgs_min):\n",
    "#         delta = len(imgs_maj) - len(imgs_min)\n",
    "#         np.random.seed(seed_val)\n",
    "#         rand_ind=np.random.randint(0,len(imgs_min),delta)\n",
    "#         imgs_min =  np.concatenate(( imgs_min, imgs_min[rand_ind]))\n",
    "#         return imgs_maj, imgs_min\n",
    "    \n",
    "#     if len(train_images_0) > len(train_images_1):\n",
    "#         train_images_0, train_images_1 = euql_cls(train_images_0, train_images_1)\n",
    "             \n",
    "#     if len(train_images_1) > len(train_images_0):\n",
    "#         train_images_1, train_images_0 = euql_cls(train_images_1, train_images_0)\n",
    "\n",
    "p_label = 1 # 1\n",
    "n_label = 0 # 0\n",
    "\n",
    "# normalize images\n",
    "# mu_images = np.mean(np.concatenate(( train_images_0 , train_images_1)))\n",
    "# sigma_images = np.sqrt(np.var(np.concatenate(( train_images_0 , train_images_1))))\n",
    "# train_images_0 = (train_images_0 - mu_images) / sigma_images\n",
    "# train_images_1 = (train_images_1 - mu_images) / sigma_images\n",
    "# train_images_0 = train_images_0 - train_images_0.min()\n",
    "# train_images_0 = train_images_0/train_images_0.max()\n",
    "# train_images_1 = train_images_1 - train_images_1.min()\n",
    "# train_images_1 = train_images_1/train_images_1.max()\n",
    "\n",
    "\n",
    "train_images = np.concatenate(( train_images_0 , train_images_1)).astype(\"float32\") / 255 \n",
    "train_labels = np.concatenate(( n_label*np.ones(len(train_images_0)) , p_label*np.ones(len(train_images_1)) ))\n",
    "\n",
    "## test \n",
    "test_images_0 = images_0_test \n",
    "test_images_1 = images_1_test \n",
    "test_images = np.concatenate(( test_images_0, test_images_1)).astype(\"float32\") / 255\n",
    "# test_images = (test_images - mu_images) / sigma_images\n",
    "# test_images = test_images - test_images.min()\n",
    "# test_images = test_images/test_images.max()\n",
    "\n",
    "test_labels = np.concatenate(( n_label*np.ones(len(test_images_0)) , p_label*np.ones(len(test_images_1)) ))\n",
    "\n",
    "test_names_0 = names_0_test \n",
    "test_names_1 = names_1_test   \n",
    "test_names = np.concatenate(( test_names_0,     test_names_1))\n",
    "\n",
    "# shuffle\n",
    "(X_train, y_train) = shuffle(train_images, train_labels)\n",
    "(X_test, y_test, X_test_names) = shuffle(test_images, test_labels, test_names)\n",
    "\n",
    "X_train = np.expand_dims(X_train,-1)\n",
    "X_train = X_train.repeat(3, -1)\n",
    "\n",
    "X_test = np.expand_dims(X_test,-1)\n",
    "X_test = X_test.repeat(3, -1)\n",
    "\n",
    "\n",
    "\n",
    "print('')\n",
    "print('train_images_0', train_images_0.shape , train_images_0.min(), train_images_0.max(), np.mean(train_images_0), np.var(train_images_0) )\n",
    "print('train_images_1', train_images_1.shape , train_images_1.min(), train_images_1.max(), np.mean(train_images_1), np.var(train_images_1) )\n",
    "\n",
    "print('')\n",
    "print('X_train', X_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('X_test', X_test.shape , X_test.min(), X_test.max(), np.mean(X_test), np.var(X_test) )\n",
    "\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(np.expand_dims(train_images_0,-1), 2,10)\n",
    "print('')\n",
    "sample_images(np.expand_dims(train_images_1,-1), 2,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(train_images_0.flatten(), 100, alpha=0.5)\n",
    "_ = plt.hist(train_images_1.flatten(), 100, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def brightness(x):\n",
    "    x = tf.image.random_brightness(x, 0.20)\n",
    "    x = tf.clip_by_value(x, 0, 1)\n",
    "    return x\n",
    "@tf.function\n",
    "def contrast(x):\n",
    "    x = tf.image.random_contrast(x, 0.80, 1.20)\n",
    "    x = tf.clip_by_value(x, 0, 1)\n",
    "    return x\n",
    "@tf.function\n",
    "def rotate(x):\n",
    "    orient = np.random.choice([1,2,3], 1)[0] # 90,180,270\n",
    "    x = tf.image.rot90(x, k=orient)\n",
    "    # x = tf.contrib.image.rotate(y, angles=2.356194490192345, interpolation='NEAREST')\n",
    "    return x\n",
    "@tf.function\n",
    "def flip_ver(x):\n",
    "    x = tf.image.random_flip_up_down(x)\n",
    "    return x\n",
    "@tf.function\n",
    "def flip_hor(x):\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x\n",
    "@tf.function\n",
    "def add_noise(x):\n",
    "    x_noise = tf.random.normal(shape=(image_size, image_size, 1), mean=0.0, stddev=0.05, dtype=tf.float32)\n",
    "    x = tf.add(x, x_noise) \n",
    "    x = tf.clip_by_value(x, 0, 1)\n",
    "    return x\n",
    "@tf.function\n",
    "def resize_crop(x):\n",
    "    rand_size = tf.random.uniform(shape=[],  minval=int(0.85 * image_size),  maxval=1 * image_size, dtype=tf.int32,)\n",
    "    crop = tf.image.random_crop(x, (rand_size, rand_size, x.shape[-1]))\n",
    "    x = tf.image.resize(crop, (image_size, image_size))\n",
    "    return x\n",
    "@tf.function\n",
    "def blur(x):\n",
    "    # s = np.random.random()\n",
    "    # x = tfa.image.gaussian_filter2d(image=x, sigma=s)     \n",
    "    x = tfa.image.gaussian_filter2d(x, filter_shape=(3,3), sigma=0.3)\n",
    "    return x\n",
    "\n",
    "def augment_img(img):\n",
    "    img_aug = np.copy(img)\n",
    "    ### flip hor\n",
    "    # rand_val = np.random.uniform(0,1)\n",
    "    # if rand_val > 0.5:\n",
    "    #     img_aug = flip_hor(img_aug)\n",
    "    ### flip ver\n",
    "    # rand_val = np.random.uniform(0,1)\n",
    "    # if rand_val > 0.5:\n",
    "    #     img_aug = flip_ver(img_aug)\n",
    "    ### rotate\n",
    "    # rand_val = np.random.uniform(0,1)\n",
    "    # if rand_val > 0.5:\n",
    "    #     img_aug = rotate(img_aug)\n",
    "    ### resize crop\n",
    "    # rand_val = np.random.uniform(0,1)\n",
    "    # if rand_val > 0.5:\n",
    "    #     img_aug = resize_crop(img_aug)\n",
    "        \n",
    "    ### blur\n",
    "    rand_val = np.random.uniform(0,1)\n",
    "    if rand_val > 0.5:\n",
    "        img_aug = blur(img_aug)\n",
    "    ### brightness\n",
    "    rand_val = np.random.uniform(0,1)\n",
    "    if rand_val > 0.5:\n",
    "        img_aug = brightness(img_aug)\n",
    "    ### contrast\n",
    "    rand_val = np.random.uniform(0,1)\n",
    "    if rand_val > 0.5:\n",
    "        img_aug = contrast(img_aug)\n",
    "    ### noise\n",
    "    rand_val = np.random.uniform(0,1)\n",
    "    if rand_val > 0.5:\n",
    "        img_aug = add_noise(img_aug)\n",
    "    return img_aug\n",
    "\n",
    "\n",
    "def augment_imgs(imgs):\n",
    "    imgs_aug = []\n",
    "    for i in range(len(imgs)):\n",
    "        img_aug = augment_img(imgs[i])\n",
    "        imgs_aug.append( img_aug )\n",
    "    imgs_aug = np.array(imgs_aug)\n",
    "    return imgs_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf_model == 'resnet': \n",
    "    model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(img_size, img_size,3)) # Xception ResNet50\n",
    "if clf_model == 'vgg': # \"imagenet\" or None\n",
    "    model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(img_size, img_size,3)) # None , \"imagenet\"\n",
    "if clf_model == 'densenet': \n",
    "    model = DenseNet121(weights=\"imagenet\", include_top=False, input_shape=(img_size, img_size,3)) # Xception ResNet50    \n",
    "if clf_model == 'simple_cnn':\n",
    "    input_D = Input(shape=(img_size, img_size, 3))\n",
    "    X = Conv2D(filters=128, kernel_size=5, strides=2, activation='relu', padding='same')(input_D) #### HP #####\n",
    "    X = Conv2D(filters=128, kernel_size=7, strides=1, activation='relu', padding='same')(X)\n",
    "    X = Dropout(0.15)(X)\n",
    "    X = Conv2D(filters=256, kernel_size=5, strides=2, activation='relu', padding='same')(X)\n",
    "    X = Conv2D(filters=256, kernel_size=7, strides=1, activation='relu', padding='same')(X)\n",
    "    X = Dropout(0.15)(X)\n",
    "    X = Conv2D(filters=512, kernel_size=5, strides=2, activation='relu', padding='same')(X)\n",
    "    X = Conv2D(filters=512, kernel_size=7, strides=1, activation='relu', padding='same')(X)\n",
    "    X = Dropout(0.15)(X)\n",
    "#     X = BatchNormalization()(X)\n",
    "#     X = Dropout(0.1)(X)\n",
    "    model = Model(input_D, X)\n",
    "#####################################################################################################\n",
    "if retrain_model==0:\n",
    "    for layer in model.layers: # retrain all layers\n",
    "        layer.trainable = False \n",
    "if retrain_model==1:\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True \n",
    "######################################################################################################\n",
    "# flatten and add dense layers\n",
    "X = model.output\n",
    "X = Flatten()(X)\n",
    "X = Dense(16, activation='relu')(X)  #### HP #####\n",
    "X = Dropout(0.5)(X)\n",
    "output_sigmoid = Dense(1, activation=\"sigmoid\")(X)\n",
    "# output_sigmoid = activations.sigmoid(output_linear)\n",
    "final_model = Model(inputs=model.inputs, outputs=output_sigmoid)\n",
    "######################################################################################################\n",
    "# opt = Adam(0.000001) #### HP #####\n",
    "# final_model.compile( loss=BinaryCrossentropy(label_smoothing = 0.1), optimizer=opt, metrics=[tf.keras.metrics.AUC(), 'accuracy'] ) # \n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 300\n",
    "LEARNING_RATE = 0.0000001\n",
    "optimizer=Adam(LEARNING_RATE)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.05)\n",
    "\n",
    "def bce_loss(y_real, y_pred):\n",
    "    bce_losses = -1* ( y_real*tf.math.log(y_pred) + (1-y_real)*(tf.math.log(1 - y_pred)) ) # positive value\n",
    "    # bce_losses = bce_losses[bce_losses>0.5]\n",
    "    return tf.math.reduce_mean( bce_losses )\n",
    "\n",
    "def calc_metrices(y_real, y_pred):\n",
    "        clf_loss = bce(y_real, y_pred).numpy()\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_real , y_pred)\n",
    "        auc_score = np.round(metrics.roc_auc_score(y_real , y_pred),2)\n",
    "        gmeans = np.sqrt(tpr * (1-fpr))\n",
    "        ix = np.argmax(gmeans) # locate the index of the largest g-mean\n",
    "        gmax = np.round(gmeans[ix],2)\n",
    "        th = np.round(thresholds[ix],2)\n",
    "        tpr = np.round(tpr[ix],2)\n",
    "        tnr = np.round(1-fpr[ix],2)\n",
    "        \n",
    "        y_pred_TH = np.copy(y_pred)\n",
    "        y_pred_TH[y_pred>th] = 1\n",
    "        y_pred_TH[y_pred<th] = 0\n",
    "        acc = metrics.accuracy_score(y_real, y_pred_TH)\n",
    "        f1 = f1_score(y_real, y_pred_TH)\n",
    "        return clf_loss, tnr, tpr, gmax, th, auc_score, acc, f1\n",
    "    \n",
    "def plot_hist(y_real, y_pred):\n",
    "        _ = plt.hist(y_pred[y_real==0] , 100, alpha = 0.5, color= 'r')\n",
    "        _ = plt.hist(y_pred[y_real==1] , 100, alpha = 0.5, color= 'b')\n",
    "        # plt.title('Quality scores based on real images', fontsize=20)\n",
    "        handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [\"red\", \"blue\"] ]\n",
    "        labels= [\"low\", \"high\"]\n",
    "        plt.legend(handles, labels)\n",
    "        plt.xlabel(\"classifier scores\", fontsize=15)\n",
    "        plt.ylabel(\"# of samples\", fontsize=15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = np.copy(X_test)\n",
    "idx = np.random.randint(0, X_train.shape[0], 2000)\n",
    "train_imgs = np.copy(X_train[idx])\n",
    "train_y = y_train[idx]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch\", epoch)\n",
    "    iteration = 0\n",
    "    for n_batch in range(len(X_train) // batch_size): \n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        image_batch = np.copy(X_train[idx])\n",
    "        y_batch = np.copy(y_train[idx])\n",
    "        y_batch = tf.convert_to_tensor(y_batch.astype('float32'))\n",
    "        if aug_data_flag==1:\n",
    "            image_batch = augment_imgs(image_batch) # \n",
    "        with tf.GradientTape() as tape:\n",
    "            clf_pred = final_model(image_batch, training=True)\n",
    "            clf_loss = bce(y_batch, clf_pred)\n",
    "        trainable_variables = final_model.trainable_variables\n",
    "        clf_gradients = tape.gradient(clf_loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(clf_gradients, trainable_variables))\n",
    "            \n",
    "    # end of epoch - test \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "    \n",
    "        print('')\n",
    "        train_clf_pred = final_model.predict(train_imgs)[:,0]    \n",
    "        test_clf_pred = final_model.predict(test_imgs)[:,0]\n",
    "\n",
    "        train_clf_loss, train_tnr, train_tpr, train_gmax, train_th, train_auc, train_acc, train_f1 = calc_metrices(train_y, train_clf_pred)\n",
    "        test_clf_loss, test_tnr, test_tpr, test_gmax, test_th, test_auc, test_acc, test_f1 = calc_metrices(y_test, test_clf_pred)\n",
    "\n",
    "        print('train', '                 ','test')\n",
    "        print('loss', train_clf_loss, '    ', test_clf_loss)\n",
    "        print('acc', train_acc, '    ', test_acc)\n",
    "        print('gmax', train_gmax, '    ', test_gmax)\n",
    "        print('th', train_th, '    ', test_th)\n",
    "        print('tnr', train_tnr, '    ', test_tnr)\n",
    "        print('tpr', train_tpr, '    ', test_tpr)\n",
    "        print('auc', train_auc, '    ', test_auc)\n",
    "        print('f1', train_f1, '    ', test_f1)\n",
    "        print('')\n",
    "    \n",
    "    \n",
    "        plot_hist(train_y, train_clf_pred)\n",
    "        plot_hist(y_test, test_clf_pred)\n",
    "    \n",
    "    final_model.save(domain_path + 'saved_CLF/'+ clf_model + '_seed_' + str(seed_val) +  '_epoch_'  + str(epoch) + '_auc_' + str(test_auc) + '_gmax_' + str(np.round(test_gmax,2)) + '_tpr_' + str(np.round(test_tpr,2)) + '_tnr_' + str(np.round(test_tnr,2)) + '_acc_' + str(np.round(test_acc,2)) + '_f1_' + str(np.round(test_f1,2)) + '.keras', include_optimizer=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DISCOVERenv2",
   "language": "python",
   "name": "discoverenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
